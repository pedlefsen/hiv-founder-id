\documentclass[a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{tabularx,ragged2e,booktabs,caption}
\newcolumntype{C}[1]{>{\Centering}m{#1}}
\renewcommand\tabularxcolumn[1]{C{#1}}

\title{DS Poisson Fitter}
\author{Paul Edlefsen}
\date{}

\begin{document}

\maketitle

<<echo = FALSE>>=
## R packages needed
library( "xtable" )
library( "coin" )
library( "ggplot2" )

## for prettyPrintPValuesTo4Digits
source( "~/src/from-git/projects/pedlefse/rapporttemplates/rapporttemplates-util.R" )

# Setup for prettier Sweave output.
old.continue.option <- options( continue = " " )
@ 

<<echo = FALSE>>=
#### ERE I AM, there is still a problem with the plausibility calculation being unreasonably high when we get into the twilight zone where numeric stuff fails.  Particularly the problem is in logSubtract returning -Inf when its arguments are different but very close.

  # The trick for subtracting small logged doubles in log space, with logX > logY
  logSubtract <- Vectorize( function (logX, logY) { # modified from: https://facwiki.cs.byu.edu/nlp/index.php/Log_Domain_Computations
      #print( paste( "logSubtract", logX, logY ) );
       if( !is.finite( logX ) ) {
           return( logX );
       }
       negDiff <- logY - logX;
       if( negDiff < -20 ) {
           return( logX );
       }
       exp.negDiff <- exp( negDiff );
       if( exp.negDiff == 1 ) {
           return( - logY ); # most of the difference is just Y.
           #return( -Inf );
       }
       # 4. otherwise use some nice algebra to stay in the log domain
       #    (except for negDiff)
      .rv <- logX + log( 1.0 - exp( negDiff ) );
      #print( paste( "RV:", .rv ) );
       return( .rv );
   } ); # logSubtract (modified from https://facwiki.cs.byu.edu/nlp/index.php/Log_Domain_Computations)
PoissonDSM.calculatePlausibilityOfSingleton.one.obs <- Vectorize(
    function( x, k, log.result = FALSE ) {
        .rv <- pgamma( x, shape = k ) - pgamma( x, shape = k + 1 );
        if( ( .rv == 0 ) || log.result ) {
            .rv <- log( .rv );
            if( any( !is.finite( .rv ) ) ) {
                .rv.log.part1 <- pgamma( x, shape = k, log.p = TRUE );
                .rv.log.part2 <- pgamma( x, shape = ( k + 1 ), log.p = TRUE );
                .rv <- logSubtract( .rv.log.part1, .rv.log.part2 );
                if( any( !is.finite( .rv ) ) ) {
                    .rv.hack <- 
                    # Very special hack:
                    # when k is 0, we get underflow that can be recovered using the special trick that logSubtract( 0, pgamma( x, shape = 1, log.p = TRUE ) ) is approximately exp( -x - 2*k ).
                    .rv <- exp( -x - 2*k );
                    print( paste( "HACK:", x, k, .rv ) );
                }
                .rv <- ifelse( is.finite( .rv ), .rv, .rv.hack );
            }
            if( !log.result ) {
                .rv <- exp( .rv );
            }
        }
        return( .rv );
    }, vectorize.args = "k" );
PoissonDSM.calculatePlausibilityOfSingletons <- function ( xs, observed.counts, log.result = FALSE, scale.result.by.log = 0 ) {
    .mat <-
        PoissonDSM.calculatePlausibilityOfSingleton.one.obs( xs, observed.counts, log.result );
    if( is.null( dim( .mat ) ) ) {
        .mat <- t( as.matrix( .mat ) );
    }
    if( log.result ) {
        .log.plaus <- apply( .mat, 1, sum );
        names( .log.plaus ) <- xs;
        if( is.numeric( scale.result.by.log ) && ( scale.result.by.log != 0 ) ) { .log.plaus <- ( .log.plaus + scale.result.by.log ); }
        return( .log.plaus );
    } else {
        .plaus <- apply( .mat, 1, prod );
        names( .plaus ) <- xs;
        if( is.numeric( scale.result.by.log ) && ( scale.result.by.log != 0 ) ) { .log.plaus <- ( .log.plaus * exp( scale.result.by.log ) ); }
        return( .plaus );
    }
} # PoissonDSM.calculatePlausibilityOfSingletons (..)
    
# But now we want the hpd
PoissonDSM.calculatePlausibilityOfSingletons.integratedPlausibility <- function( .dta, log.result = FALSE ) {
    if( log.result ) {
        # First get a sense of what the scale factor should be; look at the peak.
        .scale.factor <- 0 - PoissonDSM.calculatePlausibilityOfSingletons( mean( .dta ), .dta, log.result = TRUE, scale.result.by.log = 0 );
        # Sometimes if the data really poorly fit, the plausibility is 0 at what should be the peak, due to numerical instabilities.  In that case we search for the maximum.
        if( !is.finite( .scale.factor ) ) {
            .scale.factor = 0 - suppressWarnings( optimize( f = function( .x ) { PoissonDSM.calculatePlausibilityOfSingletons( .x, .dta, log.result = TRUE, scale.result.by.log = 0 ) }, lower = 0, upper = mean( .dta ), maximum = T ) )$objective;
        }
        print( paste( "SCALE FACTOR:", .scale.factor ) );
        .integrated.fn <- function( x ) {
            .log.rv <- PoissonDSM.calculatePlausibilityOfSingletons( x, .dta, log.result = TRUE, scale.result.by.log = .scale.factor );
           # print( paste( x[1], exp( .log.rv[1] ) ) );
            exp( .log.rv );
        };
        Vectorize( function( y ) { log( ( integrate( f = .integrated.fn, lower = 0, upper = y )$value ) ) - .scale.factor } ); 
    } else {
        Vectorize( function( y ) { integrate( f = function( x ) { PoissonDSM.calculatePlausibilityOfSingletons( x, .dta ) }, lower = 0, upper = y )$value } ) 
    }
} # PoissonDSM.calculatePlausibilityOfSingletons.integratedPlausibility (..)

################################################################################## ten channel data generation
# ten.channel.gen.count <- 100;
# ten.channel.gen.truth.lambdas <- 1:ten.channel.gen.count;
# ten.channel.gen.outrageouslyhighvalue <- ten.channel.gen.truth.lambdas + 4 * sqrt( ten.channel.gen.truth.lambdas ); # The error increases a lot if this is too high.  I think it should be no more than 20 times the true lambda.  TODO: Detect it somehow.
# 
# ten.channel.gen.n <- t( apply( as.array( 1:ten.channel.gen.count ), 1, function( i ) { rpois( 10, ten.channel.gen.truth.lambdas[ i ] ) } ) );
# rownames( ten.channel.gen.n ) <- ten.channel.gen.truth.lambdas;
# 
# # estimates using the mode of the plausibility transform are the same as the naive estimate (mean):
# # First 26 use lower upper bound for search or else it gets lost
# stopifnot( all.equal( apply( ten.channel.gen.n[1:26, ], 1, function( .dta ) { optimize( function( x ) { PoissonDSM.calculatePlausibilityOfSingletons( x, .dta ) }, lower = 0, upper = 40, maximum = T, tol = 1E-5 )$maximum } ), apply( ten.channel.gen.n[1:26, ], 1, function( .dta ) { mean( .dta ) } ), tolerance = 1E-5 ) )
# # Now the rest
# stopifnot( all.equal( apply( ten.channel.gen.n[27:100, ], 1, function( .dta ) { optimize( function( x ) { PoissonDSM.calculatePlausibilityOfSingletons( x, .dta ) }, lower = 0, upper = 200, maximum = T, tol = 1E-5 )$maximum } ), apply( ten.channel.gen.n[27:100, ], 1, function( .dta ) { mean( .dta ) } )  ) )

# ten.channel.gen.normalizedPlausibilities <- function ( k ) {
#     ten.channel.gen.integratedPlausibility.totalplaus <- PoissonDSM.calculatePlausibilityOfSingletons.integratedPlausibility( ten.channel.gen.n[ k, ] )( ten.channel.gen.outrageouslyhighvalue[ k ] );
#     return( Vectorize( function( y ) { PoissonDSM.calculatePlausibilityOfSingletons.integratedPlausibility( ten.channel.gen.n[ k, ] )( y ) / ten.channel.gen.integratedPlausibility.totalplaus } ) );
# }
# ##  NOTE HACK using upper = ten.channel.gen.outrageouslyhighvalue
# ten.channel.gen.quantile <- function( k ) { function( quantile ) { optimize( function( y ) { abs( ten.channel.gen.normalizedPlausibilities( k )( y ) - quantile ) }, lower = 0, upper = ten.channel.gen.outrageouslyhighvalue[ k ] )$minimum } }
# 
# ten.channel.gen.quantiles.by.lambda <- t( sapply( truth.lambdas, function( k ) { c( ten.channel.gen.quantile( k )( 0.025 ), ten.channel.gen.quantile( k )( 0.975 ) ) } ) );
# colnames( ten.channel.gen.quantiles.by.lambda ) <- c( "2.5%", "97.5%" );

####==== From PFitter.R

library(tools)
## TODO: DEHACKIFY
args = #commandArgs(trailingOnly=TRUE)
# c( "~/src/from-git/hiv-founder-id/Abrahams-2009aa-hiv-founder-id_resultDir/0334_fixHypermutatedSequences_PoissonFitterDir/0334_fixHypermutatedSequences_pairwiseHammingDistances.txt",
    c( "~/src/from-git/hiv-founder-id/Abrahams-2009aa-hiv-founder-id_resultDir/0478_fixHypermutatedSequences_removeRecombinedSequences_PoissonFitterDir/0478_fixHypermutatedSequences_removeRecombinedSequences_pairwiseHammingDistances.txt",
   "2.16e-05", "2517" );
infile <- args[1]
epsilon <- c(as.numeric(args[2]))
nbases <- c(as.numeric(args[3]))

dir <- paste(dirname(infile), '/', sep='')
#outfile <- paste(dir, "DS.LOG_LIKELIHOOD.results.txt", sep="")
#outfile2 <- paste(dir, "DS.CONVOLUTION.results.txt", sep="")

cat( "DS PFitter.." );

### FUNCTIONS ###
phi <- sqrt(1+4/3)
days <- function(l,nb,epsilon) 1.5*((phi)/(1+phi))*(l/(epsilon*nb) - (1-phi)/(phi^2))
### end FUNCTIONS ###

#if(!file.exists(outfile)){
#	write(paste("Sample", "Lambda", "St.Dev", "NSeq", "NBases", "MeanHD", "MaxHD","Days(CI)", "Chi2","DF","Goodness_of_pval", sep="\t"), file=outfile, append=FALSE)
#}
	
dlist <- read.table( file=infile, sep="\t", stringsAsFactors=F )

### calc HD with consensus
d0 <- dlist[which(dlist[,1]==dlist[1,1]),]
mult0 <- as.numeric(sub('.+_(\\d+)$', '\\1', d0[,2]))
nseq <- sum(mult0)
yvec0 <- rep(0, (1+max(d0[,3])))
for(i in 1:(1+max(d0[,3]))){ yvec0[i] <- sum(mult0[which(d0[,3]==(i-1))]) }

nl0 <- length(yvec0);
clambda <- sum((1:(nl0-1))*yvec0[-1])/sum(yvec0) #### THIS IS THE LAMBDA THAT FITS THE CONSENSUS ONLY DISTRIBUTION

### calc intersequence HD
d1 <- dlist[-which(dlist[,1]==dlist[1,1]),]	
yvec <- rep(0, (1+max(d1[,3])))
seqnames <- unique(c( d1[,1], d1[,2] ))
for(i in 1:length(seqnames)){
    tmp <- d1[which(d1[,1]==seqnames[i]),,drop = FALSE]
    if( nrow( tmp ) == 0 ) {
        next;
    }
	m0 <- as.numeric(sub('.+_(\\d+)$', '\\1', tmp[1,1]))
	yvec[1] <- yvec[1] + 0.5*m0*(m0-1) ## 0 bin
	for(j in 1:dim(tmp)[1]){
		m1 <- as.numeric(sub('.+_(\\d+)$', '\\1', tmp[j,2]))
		val <- tmp[j,3]
		yvec[val+1] <- yvec[val+1] + m0*m1
	}
}

## U STAT from PFitter.R
calculateUSTATvarHD <- function () {
  #### U STAT ESTIMATE OF ST DEV
  #### FORMULAE
  #### Var(HD) = (N(N-1)/2)^(-1) (2(N-2)sigma1^2 + sigma2^2)
  #### sigma1^2 = (N(N-1)(N-2)/3 -1)^(-1) sum_{i<j<l} ((Dij-mu)(Dil-mu)+(Dij-mu)(Djl-mu))
  #### sigma2^2 = (N(N-1)/2-1)^(-1) sum_{i<j} (Dij-mu)^2
  
  ### construct a matrix of Dij's
  TX <- matrix(rep(NA,nseq^2), ncol=nseq)
  rownames( TX ) <- seqnames;
  colnames( TX ) <- seqnames;
  for(i in 1:(dim(d0)[1]-1)){
  	useq <- d0[i,2]
  	TX[d1[which(d1[,1]==useq),2],i] <- d1[which(d1[,1]==useq),3];
  }
  
  sigma1 <- 0
  sigma2 <- 0
  muhat <- 0
  denmu <- (sum( !is.na( TX )))^(-1)
  ## TODO: Figure out what (if any) is the right fix to the below to handle sparse distances
  den1 <- 12*(nseq*(nseq-1)*(nseq-2)*(nseq-3))^(-1)  
  den2 <- den1/4
  
  for(n in 1:(nseq-1)){
      for(m in (n+1):nseq){
          if( !is.na( TX[ m, n ] ) ) {
              muhat <- muhat + mult0[n]*mult0[m]*denmu*TX[m,n]
  	}
      }
  }
  
  for(n in 1:nseq){
  	dnn <- 0
  	sigma1 <- sigma1 + choose(mult0[n],3)*den1*2*(dnn-muhat)^2 
  	sigma2 <- sigma2 + choose(mult0[n],2)*den2*(dnn-muhat)^2
  	if(n != nseq){
  		for(m in (n+1):nseq){
                      dnm <- TX[m,n]
                      if( !is.na( dnm ) ) {
  			dmm <- 0
  			sigma2 <- sigma2 + mult0[n]*mult0[m]*(dnm - muhat)^2
  			sigma1 <- sigma1 + (2/3)*choose(mult0[n],2)*mult0[m]*(dnm-muhat)*(dnm+2*dnn-3*muhat)
  			sigma1 <- sigma1 + (2/3)*mult0[n]*choose(mult0[m],2)*(dnm-muhat)*(dnm+2*dmm-3*muhat)
  			if(m != nseq){
  				for(l in (m+1):nseq){
  					dnl <- TX[l,n]
  					dlm <- TX[l,m]
                                          if( !is.na( dnl ) && !is.na( dlm ) ) {
                                              sigma1 <- sigma1 + (2/3)*mult0[n]*mult0[m]*mult0[l]*((dnm-muhat)*(dnl-muhat)+(dnm-muhat)*(dlm-muhat)+(dnl-muhat)*(dlm-muhat))
                                          }
  				}
  			}
  		    } # End if !is.na( dnm )
                  } # End for( m )
   	}
  }
  
  ## varhd <- sqrt(denmu*(2*(nseq-2)*sigma1 + sigma2))
  A <- 8/(nseq*(nseq-1)*(nseq-2)*(nseq-3))
  B <- 4/(nseq*(nseq-1)*(nseq-2)*(nseq-3))
  newvarhd <- sqrt(A*sigma1 + B*sigma2)
  return( newvarhd );
} # calculateUSTATvarHD (..)
	
### Fitting
nl <- length(yvec)
lambda <- sum((1:(nl-1))*yvec[-1])/sum(yvec)
estdays <- days(lambda, nbases, epsilon)
		
print(paste("PFitter Estimated Lambda", format(lambda, digits=4), sep=" "))

newvarhd <- calculateUSTATvarHD();
lambda.low <- lambda - 1.96*newvarhd;
lambda.high <- lambda + 1.96*newvarhd;
upplim <- days(lambda.high, nbases, epsilon)
lowlim <- days(lambda.low, nbases, epsilon)

uppdays <- round(upplim)
lowdays <- round(lowlim)
formatteddays <- paste(round(estdays), " (", lowdays, ", ", uppdays, ")", sep="") 
print(paste("PFitter Estimated Days:", formatteddays, sep=" "))


## NOW for DS
HD.outrageouslyhighvalue <- mean( d1[ , 3 ] ) + 4*sd( d1[ , 3 ] );
HD.integratedPlausibility.totalplaus <- PoissonDSM.calculatePlausibilityOfSingletons.integratedPlausibility( d1[ , 3 ] )( HD.outrageouslyhighvalue );
HD.normalizedPlausibilities <- 
    Vectorize( function( y ) { PoissonDSM.calculatePlausibilityOfSingletons.integratedPlausibility( d1[ , 3 ] )( y ) / HD.integratedPlausibility.totalplaus } );
##  NOTE HACK using upper = HD.outrageouslyhighvalue/4
HD.quantile <- function( quantile ) { optimize( function( y ) { abs( HD.normalizedPlausibilities( y ) - quantile ) }, lower = 0, upper = HD.outrageouslyhighvalue/4 )$minimum }

DS.lambda <- mean( d1[ , 3 ] );
print(paste("DS PFitter Estimated Lambda", format(DS.lambda, digits=4), sep=" "))

DS.lambda.low <- HD.quantile( 0.025 );
DS.lambda.high <- HD.quantile( 0.975 );

DS.estdays <- days(DS.lambda, nbases, epsilon)
DS.upplim <- days(DS.lambda.high, nbases, epsilon)
DS.lowlim <- days(DS.lambda.low, nbases, epsilon)

DS.uppdays <- round(DS.upplim)
DS.lowdays <- round(DS.lowlim)

DS.formatteddays <- paste(round(DS.estdays), " (", DS.lowdays, ", ", DS.uppdays, ")", sep="") 
print(paste("DS PFitter Estimated Days:", DS.formatteddays, sep=" "))

## Could add Bayesian.  Congjugate prior is Gamma, and Gamma( 1, 1 ) (expo) has log( x ) propto 1.  [However note that alpha and beta approaching zero might give the closest analogue to the DS solution.]
# Here I'm using the posterior median; the mode or mean are other options; can be looked up for generic gamma distrns.
posterior.lambda <- qgamma( .5, 1+sum( d1[ , 3 ] ), 1+nrow( d1 ) );
posterior.lambda.low <- qgamma( .025, 1+sum( d1[ , 3 ] ), 1+nrow( d1 ) );
posterior.lambda.high <- qgamma( .975, 1+sum( d1[ , 3 ] ), 1+nrow( d1 ) );
posterior.estdays <- days(posterior.lambda, nbases, epsilon)
posterior.upplim <- days(posterior.lambda.high, nbases, epsilon)
posterior.lowlim <- days(posterior.lambda.low, nbases, epsilon)

posterior.uppdays <- round(posterior.upplim)
posterior.lowdays <- round(posterior.lowlim)

posterior.formatteddays <- paste(round(posterior.estdays), " (", posterior.lowdays, ", ", posterior.uppdays, ")", sep="") 
print(paste("Bayesian PFitter Estimated Days:", posterior.formatteddays, sep=" "))

@ 

\section{Goodness of fit}
Here we use the General Univariate DSM, which consists of the DS combination of a Univariate Nonparametric DSM with the constraint that the CDF pass within some region of height $\epsilon$ around a Poisson CDF.  In any random sample of pepr Uniform variables, there is a smallest value $\epsilon$ such that the true CDF evaluated at any value is at most $\epsilon$ (cumulative probability) different from a Poisson CDF.

<<echo = FALSE>>=

## For a given set of data and corresponding set of pepr variates, compute the smallest epsilon and corresponding lambda such that the data CDF passes an average of epsilon units (vertically) from Poisson( lambda ), averaged over the observed data points.
PoissonDSM.getSmallestEpsilonAndLambda <- function ( observed.data, pepr.variates = runif( length( observed.data ) ), observed.data.are.already.in.ascending.order = FALSE, pepr.variates.are.already.in.ascending.order = FALSE, pepr.variates.are.validity.checked = FALSE, compare.to.specific.pois.with.lambda = NULL ) {
    stopifnot( length( observed.data ) > 0 );
    stopifnot( length( pepr.variates ) == length( observed.data ) );
    if( !observed.data.are.already.in.ascending.order ) {
        observed.data <- sort( observed.data );
        observed.data.are.already.in.ascending.order <- TRUE;
    }
    if( !pepr.variates.are.already.in.ascending.order ) {
        pepr.variates <- sort( pepr.variates );
        pepr.variates.are.already.in.ascending.order <- TRUE;
    }
    if( !pepr.variates.are.validity.checked ) {
        stopifnot( ( pepr.variates[ 1 ] >= 0 ) && ( pepr.variates[ 1 ] <= pepr.variates[ length( pepr.variates ) ] ) && ( pepr.variates[ length( pepr.variates ) ] <= 1 ) );
        pepr.variates.are.validity.checked <- TRUE;
    }
    if( !is.null( compare.to.specific.pois.with.lambda ) ) {
        # Force lambda for comparison.
        lambda.low <- compare.to.specific.pois.with.lambda;
        lambda.high <- compare.to.specific.pois.with.lambda;
    } else {
        lambda.low <- max( min( observed.data ), ( mean( observed.data ) - 4 * sd( observed.data ) ) );
        lambda.high <- min( max( observed.data ), ( mean( observed.data ) + 4 * sd( observed.data ) ) );
    }
    .epsilon.from.lambda.fn <- 
      function( lambda ) {
        # Here we use the assumption that the observed data are in ascending order, which implies that the quantiles are, too.
        .quantiles <- ppois( observed.data, lambda );
        .abs.diffs <- abs( pepr.variates - .quantiles );
        # DON'T Return the largest difference.
        #return( max( .abs.diffs ) );
        # Return the average difference.
        return( mean( .abs.diffs ) );
      };
    if( lambda.low == lambda.high ) {
        the.results <- list( "lambda" = lambda.low,
                         "epsilon" = .epsilon.from.lambda.fn( lambda.low ) );
    } else {
      the.results <- 
      optimize( f = .epsilon.from.lambda.fn, lower = lambda.low, upper = lambda.high, maximum = FALSE, tol = 1E-5 );
      names( the.results ) <- c( "lambda", "epsilon" );
    }
    return( the.results );
} # PoissonDSM.getSmallestEpsilonAndLambda ( observed.data, pepr.variates )

@ 

The gamma-distributed lower ends of the lambda intervals contributed by each observation are independent from the exponentially-distributed interval lengths; conflict arises across the intervals whenever the largest lower-end exceeds the largest upper-end (sum of lower end and interval length).  If we allow for the weakening assumption that the assignment of interval lengths to interval ends is ``not known'', then condition occurs whenever the distance between the largest and smallest of the lower-ends exceeds the largest interval length.  Since they are independent, these can be described analytically. The probability distribution of the largest of the interval lengths is simply that of the maximum of exponential distributions (oddly enough in this case of unit exponentials, the maximum of two independent unit exponentials is the same as the distribution of the sum of one exponential and half of one independent exponential; so by induction the maximum of three or more exponentials is a sum with coefficients that are diminishing geometric series).  And the probability that the maximum difference in the lower ends exceeds a fixed value $c$ can be written as
\begin{equation}
  \Prob{ \bigl( \max_{i,j}( \Lambda_l^{i} - \Lambda_l^{j} ) > c \bigr) } = \\
  \Prob{ \Bigl( \bigl( \max_{i}( \Lambda_l^{i} ) - \min_{j}( \Lambda_l^{j} ) \bigr) > c \Bigr) } \\hbox{,}
\end{equation} 
but these maximum and minimum values are not independent.  We use the law of total probability, 
\begin{equation}
  \int_x { \Prob{ \Bigl( \bigl( \max_{i-\min}( \Lambda_l^{i} ) \bigr) > ( c + x ) \Bigr) } dF\Lambda_l^{\min} } = \\
  \int_x { ( 1 - F\Lambda_l^{\max|\min=x}( c + x ) ) dF\Lambda_l^{\min}(x) } \hbox{,}
\end{equation}
where $F\Lambda_l^{\min}$ is the CDF of the minimum of the Gamma-distributed lower ends of the lambda interval, and where, given that the minimum is $\Lambda_l^{\min} = x$, $F\Lambda_l^{\max|\min=x}$ is the CDF of the maximum.

So we can compute the probability of conflict as an expectation over the minimal interval width on right-hand side, $c$, like so:
\begin{equation}
  \Prob{ \bigl( \max_{i,j}( \Lambda_l^{i} - \Lambda_l^{j} ) > \min_k( \Lambda_w^{k} \bigr) ) } = \\
  \int_c { \Prob{ \bigl( \max_{i,j}( \Lambda_l^{i} - \Lambda_l^{j} ) > c ) } dF\Lambda_w^{\min} } \hbox{,}
\end{equation}
where $F\Lambda_w^{\min}$ is the CDF of the minimum of the interval widths.  As noted above, $\Lambda_w^{\min}$ is exponentially distributed.

Next we want to know the evidence about two distributions being the same, since it turns out that the pfitter test is actually comparing the lambda estimated from the pairwise distances among the sequence to the lambda estimated between each sequence and the consensus sequence.  If we pretend these are independent, then the expectation is that a starlike phylogeny should have all paths passing through the center, so the poisson distribution of HD among seqs should have twice the mean as the dist to the consensus (since everything has to go to the consensus at the center then out again).  That's the idea.

So now we want a P,Q,R for the assertion that the HD to the consensus is half the HD among the sequences, via a Poisson assumption.  That is, we assume that they are Poisson distributed (and that the evidence is independent, though it is not), and we ask whether the evidence supports the 2x assertion.  This should be doable analytically, but here is a numerical approach:

<<echo = FALSE, results = hide>>=
draw.one.nonconflicted.pepr.sample <- function ( sorted.observed.data, conflict.max = 5000 ) {
    ## The naive way, just keep drawing until one works.
##    if( method == "naive" ) {
      found.it <- FALSE;
      conflict <- 0;
      while( !found.it && conflict < conflict.max ) {
        # Directly draw from the Poisson DSM
        lambda.low <-
            rgamma( n = length( sorted.observed.data ), shape = sorted.observed.data );
        lambda.width <-
            rgamma( n = length( sorted.observed.data ), shape = 1 );
        if( max( lambda.low ) > ( min( lambda.low ) + max( lambda.width ) ) ) {
            ## CONFLICT.
            conflict <- conflict + 1;
            cat( "." );
            next;
        }
        found.it <- 1;
      } # end while( !found.it )
      if( !found.it ) { stop( "TOO MUCH CONFLICT!" ) }
    return( list( lambda.low = lambda.low, lambda.width = lambda.width ) );
} # draw.one.nonconflicted.pepr.sample 

sorted.consensus.distances <- sort( d0[ , 3 ] );
sorted.intersequence.distances <- sort( d1[ , 3 ] );

draw.once.from.each.and.evaluate.assertion <- function( assertion.is.that.1.is.x.times.2 = 2, sorted.observed.data.1 = sorted.consensus.distances, sorted.observed.data.2 = sorted.intersequence.distances ) {
    .sample1 <- draw.one.nonconflicted.pepr.sample( sorted.observed.data.1 );
    .sample1.min = assertion.is.that.1.is.x.times.2 * min( .sample1$lambda.low + .sample1$lambda.width );
    .sample1.max = assertion.is.that.1.is.x.times.2 * max(  .sample1$lambda.low );
    print( paste( c( ".sample1.min", .sample1.min,  ".sample1.max", .sample1.max ), collapse = " " ) );
    .sample2 <- draw.one.nonconflicted.pepr.sample( sorted.observed.data.2 );
    .sample2.min = min( .sample2$lambda.low + .sample2$lambda.width );
    .sample2.max = max(  .sample2$lambda.low );
    print( paste( c( ".sample2.min", .sample2.min,  ".sample2.max", .sample2.max ), collapse = " " ) );
    if( ( .sample1.max < .sample2.min ) ||
       ( .sample1.min > .sample2.max ) ) {
        # Evidence against.
        return( list( P = 0, Q = 1, R = 0 ) );
    }
    # No evidence can be entirely in support, so if it's not against, then we are in a "don't know" situation.
    return( list( P = 0, Q = 0, R = 1 ) );
} # draw.once.from.each.and.evaluate.assertion (..)

# GET A BUNCH OF DRAWS
# MAGIC N: DS.NDRAWS
DS.NDRAWS <- 1000;
.result <- sapply( 1:DS.NDRAWS, function( .i ) { print( .i ); draw.once.from.each.and.evaluate.assertion() } );
DS.Q <- mean( unlist( .result[ "Q", ] ) );
DS.R <- mean( unlist( .result[ "R", ] ) );
stopifnot( DS.Q + DS.R == 1 );

if( DS.Q == 0 ) {
    DS.Qtext <- paste( "<=", ( 1 / DS.NDRAWS ) );
} else {
    DS.Qtext <- sprintf( paste( "= %0.", ceiling( log10( DS.NDRAWS ) ), "f", sep = "" ), DS.Q )
}
print( paste( "The DS evidence against the assertion that the Poisson rate between sequences is twice the rate of sequences to the consensus is Q ", DS.Qtext, ". The remaining evidence (", sprintf( paste( "%0.", ceiling( log10( DS.NDRAWS ) ), "f", sep = "" ), DS.R ), ") neither supports nor contradicts the assertion.", sep = "" ) );
@ 


<<echo = FALSE>>=
# (un)Setup for prettier Sweave output.
options( continue = old.continue.option$continue )
@ 

\end{document}

